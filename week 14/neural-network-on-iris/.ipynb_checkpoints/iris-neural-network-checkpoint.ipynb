{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Neural Network\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Artifical Neural Networks are quickly becoming one of the most popular and widely used mechanisms in Machine Learning and Data Analysis. In the last number of years, numerous libraries and software has been developed to equip programmers with a set of tools for modeling and analysing data in order to recognise patterns and make predictions using large data sets. In today's age of [Big Data](https://en.wikipedia.org/wiki/Big_data) it is important to try make sense of all of the data we have in society. This could range from social media pattern recognitions from anything to finance and economic trends. The reality is that today we have more data in existence than ever before and it growing at a vast and exponential rate.\n",
    "\n",
    "Artifical Neural Networks aim to mimic and replicate the neurons of a human brain and using the power of the complex mathematical functions allow us to process and model data in such a way that we can form rational assumptions on a given data set.\n",
    "\n",
    "Given the sheer amount of data out there it is important to note that data we may analyse is often subject to human error and may not always hold a valid essense of truth. For the purpose of this example we will take a look at the [Iris Data Set](https://archive.ics.uci.edu/ml/datasets/iris). \n",
    "\n",
    "More information on the data set can be found on the link provided above or on the front page [README of this repository](https://github.com/damiannolan/iris-neural-network).\n",
    "\n",
    "Throughout the notebook we aim to build an Artifical Neural Network capable of making predictions of species of Iris Flowers using [Keras](https://keras.io) - Keras is a high-level neural networks API, written in Python and capable of running on top of [Tensorflow](https://www.tensorflow.org/).\n",
    "\n",
    "So without further ado, lets get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference links:\n",
    "#    Adapted from: https://github.com/emerging-technologies/keras-iris\n",
    "#    Data set pulled from: https://gist.github.com/curran/a08a1080b88344b0c8a7\n",
    "\n",
    "# imports and preliminaries\n",
    "import csv\n",
    "import numpy as np\n",
    "from tensorflow import keras as kr\n",
    "\n",
    "# Load the Iris dataset.\n",
    "iris = list(csv.reader(open('iris-data-set.csv')))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs and Outputs\n",
    "### Data Investigation and Classification\n",
    "\n",
    "Before trying to create a model for our Neural Network we first need to investigate our data and determine what will be the inputs and what will be our outputs. The CSV file provided contains 5 columns with:\n",
    "\n",
    "- Sepal Length\n",
    "- Sepal Width\n",
    "- Petal Length\n",
    "- Petal Width\n",
    "- Species\n",
    "\n",
    "Judging by the fact that we are trying to make predictions we must split our data set into sets of:\n",
    "\n",
    "- Inputs - Numerical data values\n",
    "- Outputs - Classification of Iris Flower species\n",
    "\n",
    "\n",
    "Now that we have the data set loaded we can extract the data we need into appropriate data sets in preparation for training and testing our Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The inputs are four floats: sepal length, sepal width, petal length, petal width.\n",
    "inputs  = np.array(iris)[:,:4].astype(np.float)\n",
    "\n",
    "# Outputs are initially individual strings: setosa, versicolor or virginica.\n",
    "outputs = np.array(iris)[:,4]\n",
    "\n",
    "# Convert the output strings to ints.\n",
    "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Classification\n",
    "\n",
    "Here we are using the Keras utility `to_categorical()` to allow us to turn our output categories into binary class matrices. This is often refered to as \"One-Hot\" encoding. This is for use with categorical_crossentropy and classification of our species (setosa, versicolor and virginica). \n",
    "\n",
    "Each Species will be represented as a binary class matrix.\n",
    "\n",
    "- Setosa [1 0 0]\n",
    "- Versicolor [0 1 0]\n",
    "- Virginica [0 0 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the category integers as binary categorical vairables.\n",
    "outputs_cats = kr.utils.to_categorical(outputs_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide & Conquer \n",
    "### Splitting the data\n",
    "\n",
    "We can now randomly split the data into two sets for:\n",
    "\n",
    "- Training\n",
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the input and output data sets into training and test subsets.\n",
    "inds = np.random.permutation(len(inputs))\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "inputs_train, outputs_train = inputs[train_inds], outputs_cats[train_inds]\n",
    "inputs_test,  outputs_test  = inputs[test_inds],  outputs_cats[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Model\n",
    "\n",
    "Below we can see an example of a how a Neural Network can be visualized. Every Neural Network is made up of these three main consituents.\n",
    "\n",
    "- Input Layer\n",
    "- $x$ number of Hidden Layers\n",
    "- Output Layer\n",
    "\n",
    "![neural_net](img/neural_net.jpeg)\n",
    "\n",
    "### Keras Models\n",
    "\n",
    "Keras offers a very useful and high level API to handle creation of Neural Networks. The [Keras Sequential Model](https://keras.io/getting-started/sequential-model-guide/) is defined as a *linear stack of layers*. This is perfect for what we need to create an Artificial Neural Network consisting of Input, Output and Hidden nodes. We define our Model and add the layers to it.\n",
    "\n",
    "We are trying to create a model that will look somewhat similar to below: \n",
    "\n",
    "![iris_model](img/iris_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a neural network.\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# Add an initial layer with 4 input nodes, and a hidden layer with 16 nodes.\n",
    "model.add(kr.layers.Dense(16, input_shape=(4,)))\n",
    "# Apply the sigmoid activation function to that layer.\n",
    "model.add(kr.layers.Activation(\"sigmoid\"))\n",
    "# Add another layer, connected to the layer with 16 nodes, containing three output nodes.\n",
    "model.add(kr.layers.Dense(3))\n",
    "# Use the softmax activation function there.\n",
    "model.add(kr.layers.Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "An [Activation Function](https://en.wikipedia.org/wiki/Activation_function) in a Neural Network defines the output of a given node given its input or set of inputs. Above we applying two activation functions in separate layers.\n",
    "\n",
    "### Sigmoid\n",
    "A sigmoid function is a mathematical function having an \"S\" shaped curve (sigmoid curve). Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula:\n",
    "\n",
    "\n",
    "![sigmoid](img/sigmoid.svg)\n",
    "\n",
    "Below we see a plot of the \"S\" shaped curved or \"Sigmoid Curve\".\n",
    "\n",
    "![curve](img/Logistic-curve.svg.png)\n",
    "\n",
    "It's usage in neural network are:\n",
    "1. Activation function that transform linear inputs to nonlinear outputs.\n",
    "2. Bound output to between 0 and 1 so that it can be interpreted as a probability.\n",
    "3. Make computation easier than arbitrary activation functions.\n",
    "\n",
    "### Softmax\n",
    "\n",
    "[Softmax regression](http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/) (or multinomial logistic regression) is a generalization of logistic regression to the case where we want to handle multiple classes.\n",
    "\n",
    "Softmax regression is defined by the mathematical formula: \n",
    "\n",
    "![softmax](img/softmax.svg)\n",
    "\n",
    "Here are using Softmax to allow us to let our data flow through the hidden layers and essentially end up as one of our defined classes:\n",
    "\n",
    "- Setosa\n",
    "- Versicolor\n",
    "- Virginica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 131\n",
      "Trainable params: 131\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Display our Model using the summary function\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the Model for training and fit the training data\n",
    "\n",
    "We configure the Model using the `compile()` function defined in the [Keras Model API](https://keras.io/models/model/).\n",
    "We define an Optimizer, a Loss function and an additional metric - accuracy.\n",
    "\n",
    "So before we can use our Model for we must first train it. Using the training data subset which we extracted before we can now fit it to our Model. \n",
    "\n",
    "The goal here is for the Optimizer to essentially minimize the Loss.\n",
    "\n",
    "We fit the model passing our inputs and our expected outputs and train it across 100 \"Epochs\" or training cycles. On each iteration we improve the improve the accuracy and miniize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 75 samples\n",
      "Epoch 1/100\n",
      "75/75 [==============================] - 3s 40ms/sample - loss: 1.2156 - accuracy: 0.0000e+00\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 1.1421 - accuracy: 0.3600\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 1.0787 - accuracy: 0.3867\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 1.0143 - accuracy: 0.3867\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.9592 - accuracy: 0.4267\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.9009 - accuracy: 0.6800\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.8507 - accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.7975 - accuracy: 0.6933\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.7449 - accuracy: 0.7200\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.6997 - accuracy: 0.7200\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.6625 - accuracy: 0.8133\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.6299 - accuracy: 0.7867\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.6071 - accuracy: 0.7733\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.5847 - accuracy: 0.8533\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.5630 - accuracy: 0.8933\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.5484 - accuracy: 0.8133\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.5322 - accuracy: 0.8933\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.5190 - accuracy: 0.7600\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.5066 - accuracy: 0.8400\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.4950 - accuracy: 0.9600\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.4872 - accuracy: 0.9333\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.4749 - accuracy: 0.9733\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.4682 - accuracy: 0.9867\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.4619 - accuracy: 0.8933\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4513 - accuracy: 0.9733\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4455 - accuracy: 0.9467\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4383 - accuracy: 0.9467\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4301 - accuracy: 0.9600\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4229 - accuracy: 0.9733\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4182 - accuracy: 0.9733\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4085 - accuracy: 0.9733\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.4025 - accuracy: 0.9867\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3935 - accuracy: 0.9867\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3889 - accuracy: 0.9733\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3816 - accuracy: 0.97330s - loss: 0.4202 - accuracy: \n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3721 - accuracy: 0.9867\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3696 - accuracy: 0.9333\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3604 - accuracy: 0.9867\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3506 - accuracy: 0.9867\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3455 - accuracy: 0.9867\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3374 - accuracy: 0.9733\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3318 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3256 - accuracy: 0.9733\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3180 - accuracy: 0.9867\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3110 - accuracy: 0.9867\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3059 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.3007 - accuracy: 0.9733\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.2925 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2884 - accuracy: 0.9867\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2821 - accuracy: 0.9867\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2769 - accuracy: 0.9867\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2723 - accuracy: 0.9733\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2671 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2601 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2558 - accuracy: 0.9867\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2571 - accuracy: 0.9867\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2471 - accuracy: 0.9867\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2419 - accuracy: 0.9733\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2393 - accuracy: 0.9867\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2329 - accuracy: 0.9733\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 3ms/sample - loss: 0.2287 - accuracy: 0.9867\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2282 - accuracy: 0.9733\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2236 - accuracy: 0.9600\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2150 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2117 - accuracy: 0.9733\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2077 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.2063 - accuracy: 0.9867\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1993 - accuracy: 0.9733\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1971 - accuracy: 0.9867\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1959 - accuracy: 0.9867\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1897 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1846 - accuracy: 0.9867\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1805 - accuracy: 0.9867\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1781 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1756 - accuracy: 0.9867\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1715 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1687 - accuracy: 0.9733\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1647 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1616 - accuracy: 0.9733\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1598 - accuracy: 0.9867\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1594 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1544 - accuracy: 0.9867\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1537 - accuracy: 0.9733\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1496 - accuracy: 0.9733\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1461 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1447 - accuracy: 0.9867\n",
      "Epoch 87/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1421 - accuracy: 0.9867\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1400 - accuracy: 0.9733\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1430 - accuracy: 0.9867\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1375 - accuracy: 0.9733\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1342 - accuracy: 0.9867\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1325 - accuracy: 0.9867\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1286 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1330 - accuracy: 0.9733\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1310 - accuracy: 0.9733\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1251 - accuracy: 0.9867\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1210 - accuracy: 0.9867\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1246 - accuracy: 0.9733\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1189 - accuracy: 0.9867\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 2ms/sample - loss: 0.1197 - accuracy: 0.9867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21ab4cf8f08>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure the model for training.\n",
    "# Uses the adam optimizer and categorical cross entropy as the loss function.\n",
    "# Add in some extra metrics - accuracy being the only one.\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model using our training data.\n",
    "model.fit(inputs_train, outputs_train, epochs=100, batch_size=1, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Loss and Accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our Model we can evalate it using the test data which we extracted before. Using `evaluate()` we expect our return values of loss and accuracy for our given Test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 1s 11ms/sample - loss: 0.1314 - accuracy: 0.9600\n",
      "\n",
      "\n",
      "Loss: 0.1314\tAccuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data set.\n",
    "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\n",
    "\n",
    "# Output the accuracy of the model.\n",
    "print(\"\\n\\nLoss: %6.4f\\tAccuracy: %6.4f\" % (loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions using the Model\n",
    "\n",
    "To make predictions using our Model we must first prepare the input data to be what the model expects. Here we use a couple of Numpy functions such as `around()` and `expand_dims()` to prepare the input data for prediction.\n",
    "\n",
    "We can then pass get our prediction as a String value from `outputs_vals` which defined earlier in the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Predict the class of a single flower.\n",
    "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\n",
    "\n",
    "# print(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\n",
    "# print(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Loading the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras offers a very simplistic way to save and load your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a file for later use.\n",
    "model.save(\"iris_neural_network.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily reload the model in another script using `model = load_model(\"path_to_model.h5\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-40d648e0bbae>\", line 5, in <module>\n",
      "    if item[0] == 1:\n",
      "IndexError: invalid index to scalar variable.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'IndexError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "pred = []\n",
    "\n",
    "# Turning the [0, 0, 0] to actuall names of the flowers for the prediction variable\n",
    "for item in prediction:\n",
    "    if item[0] == 1:\n",
    "        pred.append('Setosa')\n",
    "    elif item[1] == 1:\n",
    "        pred.append('Versicolor')\n",
    "    elif item[2] == 1:\n",
    "        pred.append('Virginica')\n",
    "\n",
    "# Converting python list to Numpy array\n",
    "pred = np.array(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "\n",
    "# Turning the [0, 0, 0] to actuall names of the flowers of output test variable\n",
    "for item in outputs_test:\n",
    "    if item[0] == 1.:\n",
    "        y_test.append('Setosa')\n",
    "    elif item[1] == 1.:\n",
    "        y_test.append('Versicolor')\n",
    "    elif item[2] == 1.:\n",
    "        y_test.append('Virginica')\n",
    "\n",
    "# Converting python list to Numpy array\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-15-6a1086619631>\", line 1, in <module>\n",
      "    confusion_matrix(y_test, pred)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 253, in confusion_matrix\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 71, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 205, in check_consistent_length\n",
      "    \" samples: %r\" % [int(l) for l in lengths])\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [75, 0]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [75, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "confusion_matrix(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3326, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-bd7a13701ce3>\", line 1, in <module>\n",
      "    print(classification_report(y_test, pred))\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 1852, in classification_report\n",
      "    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\", line 71, in _check_targets\n",
      "    check_consistent_length(y_true, y_pred)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 205, in check_consistent_length\n",
      "    \" samples: %r\" % [int(l) for l in lengths])\n",
      "ValueError: Found input variables with inconsistent numbers of samples: [75, 0]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2040, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ValueError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"C:\\Users\\syeda eman\\Anaconda3\\lib\\importlib\\__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 965, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'tensorflow_core.estimator'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [75, 0]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
